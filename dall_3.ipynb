{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RavinduPabasara/Dall-3-Tests/blob/main/dall_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD1_DZnfvw6x"
      },
      "source": [
        "# make sure to enable gpu via Edit -> Notebook settings\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3Zwiv8-os-H"
      },
      "source": [
        "# Install dependencies\n",
        "\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/Jack000/DALLE-pytorch\n",
        "!git clone https://github.com/Jack000/guided-diffusion\n",
        "!pip install -e ./CLIP\n",
        "!pip install -e ./DALLE-pytorch\n",
        "!pip install -e ./guided-diffusion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CwVFyUlqjtl"
      },
      "source": [
        "# Download dalle files\n",
        "\n",
        "!curl -OL --http1.1 'https://dall-3.com/models/dalle/bpe.model'\n",
        "!curl -OL --http1.1 'https://dall-3.com/models/dalle/dalle-latest.pt'\n",
        "\n",
        "# Download vqgan files\n",
        "!curl -L -o vqgan.yaml --http1.1 'https://heibox.uni-heidelberg.de/f/b24d14998a8d4f19a34f/?dl=1'\n",
        "!curl -L -o vqgan.pt --http1.1 'https://heibox.uni-heidelberg.de/f/34a747d5765840b5a99d/?dl=1'\n",
        "\n",
        "# Download diffusion model\n",
        "!curl -OL --http1.1 'https://dall-3.com/models/guided-diffusion/256/model-latest.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wluyGol8sD0O"
      },
      "source": [
        "# imports\n",
        "\n",
        "# torch\n",
        "\n",
        "import torch\n",
        "\n",
        "from einops import repeat\n",
        "\n",
        "# vision imports\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from torchvision.transforms import functional as TF\n",
        "\n",
        "import sys\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./DALLE-pytorch')\n",
        "sys.path.append('./guided-diffusion')\n",
        "\n",
        "# dalle related classes and utils\n",
        "\n",
        "from dalle_pytorch import DiscreteVAE, OpenAIDiscreteVAE, VQGanVAE, DALLE\n",
        "from dalle_pytorch.tokenizer import tokenizer, HugTokenizer, YttmTokenizer, ChineseTokenizer\n",
        "\n",
        "from einops import rearrange\n",
        "import math\n",
        "\n",
        "# diffusion\n",
        "import gc\n",
        "import clip\n",
        "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcWeHciLtHdS"
      },
      "source": [
        "# load models\n",
        "\n",
        "tokenizer = YttmTokenizer('bpe.model')\n",
        "load_obj = torch.load('dalle-latest.pt', map_location='cpu')\n",
        "dalle_params, vae_params, weights = load_obj.pop('hparams'), load_obj.pop('vae_params'), load_obj.pop('weights')\n",
        "\n",
        "dalle_params.pop('vae', None) # cleanup later\n",
        "vae = VQGanVAE('vqgan.pt', 'vqgan.yaml')\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "dalle = DALLE(vae = vae, **dalle_params).to(device)\n",
        "dalle.load_state_dict(weights)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se_57np1FZ9b"
      },
      "source": [
        "#DALLE-pytorch generation\n",
        "generate images using dalle (this will create a number of low quality images, which we will refine using clip guided diffusion)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoLfToixww5i"
      },
      "source": [
        "text = 'a girl smiling at the camera'\n",
        "text = text.lower()\n",
        "\n",
        "top_p = 0.85\n",
        "temperature = 0.9\n",
        "batch_size = 1\n",
        "num_batches = 16\n",
        "\n",
        "text_tokens = tokenizer.tokenize([text], dalle.text_seq_len).to(device)\n",
        "text_tokens = repeat(text_tokens, '() n -> b n', b = batch_size)\n",
        "\n",
        "outputs = []\n",
        "image_tokens = []\n",
        "\n",
        "for i in range(num_batches):\n",
        "  out, tok = dalle.generate_images(text_tokens, temperature=temperature, top_p_thresh = top_p, return_tokens = True)\n",
        "  outputs.append(out)\n",
        "  for j in range(batch_size):\n",
        "    pimg = TF.to_pil_image(out[j])\n",
        "\n",
        "    print(len(image_tokens))\n",
        "    display(pimg)\n",
        "    image_tokens.append(tok[j])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XqSGPbrFTAP"
      },
      "source": [
        "#Clip guided diffusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clm6x4pfAtM6"
      },
      "source": [
        "# setup clip guided diffusion\n",
        "\n",
        "def fetch(url_or_path):\n",
        "  if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "    r = requests.get(url_or_path)\n",
        "    r.raise_for_status()\n",
        "    fd = io.BytesIO()\n",
        "    fd.write(r.content)\n",
        "    fd.seek(0)\n",
        "    return fd\n",
        "  return open(url_or_path, 'rb')\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "  if prompt.startswith('http://') or prompt.startswith('https://'):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
        "  else:\n",
        "    vals = prompt.rsplit(':', 1)\n",
        "  vals = vals + ['', '1'][len(vals):]\n",
        "  return vals[0], float(vals[1])\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "  def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "    super().__init__()\n",
        "    print(cut_size)\n",
        "    self.cut_size = cut_size\n",
        "    self.cutn = cutn\n",
        "    self.cut_pow = cut_pow\n",
        "\n",
        "  def forward(self, input):\n",
        "    sideY, sideX = input.shape[2:4]\n",
        "    max_size = min(sideX, sideY)\n",
        "    min_size = min(sideX, sideY, self.cut_size)\n",
        "    cutouts = []\n",
        "    for _ in range(self.cutn):\n",
        "      size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "      offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "      offsety = torch.randint(0, sideY - size + 1, ())\n",
        "      cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "      cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "    return torch.cat(cutouts)\n",
        "\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "  x = F.normalize(x, dim=-1)\n",
        "  y = F.normalize(y, dim=-1)\n",
        "  return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "\n",
        "def tv_loss(input):\n",
        "  \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "  input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "  x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "  y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "  return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
        "\n",
        "\n",
        "def range_loss(input):\n",
        "  return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])\n",
        "\n",
        "model_params = {\n",
        "  'attention_resolutions': '32, 16, 8',\n",
        "  'class_cond': False,\n",
        "  'diffusion_steps': 1000,\n",
        "  'rescale_timesteps': True,\n",
        "  'timestep_respacing': '1000',  # Modify this value to decrease the number of timesteps\n",
        "  'image_size': 256,\n",
        "  'learn_sigma': True,\n",
        "  'noise_schedule': 'linear',\n",
        "  'num_channels': 256,\n",
        "  'num_head_channels': 64,\n",
        "  'num_res_blocks': 2,\n",
        "  'resblock_updown': True,\n",
        "  'use_fp16': True,\n",
        "  'use_scale_shift_norm': True,\n",
        "  'emb_condition': True\n",
        "}\n",
        "\n",
        "model_config = model_and_diffusion_defaults()\n",
        "model_config.update(model_params)\n",
        "\n",
        "model, diffusion = create_model_and_diffusion(**model_config)\n",
        "model.load_state_dict(torch.load('model-latest.pt', map_location='cpu'))\n",
        "model.requires_grad_(False).eval().to(device)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "  if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
        "    param.requires_grad_()\n",
        "\n",
        "if model_config['use_fp16']:\n",
        "  model.convert_to_fp16()\n",
        "\n",
        "def set_requires_grad(model, value):\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = value\n",
        "\n",
        "clip_model, clip_preprocess = clip.load('ViT-B/16', jit=False)\n",
        "clip_model.eval().requires_grad_(False).to(device)\n",
        "clip_size = clip_model.visual.input_resolution\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uujkOddiFI4_"
      },
      "source": [
        "# Edit settings and run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB63psmt40tv"
      },
      "source": [
        "image_index = 0             # choose the image you want to refine\n",
        "clip_guidance = False       # clip guidance is not actually necessary. Turning on clip guidance helps improve image quality but will be much slower\n",
        "diffusion_batch_size = 1\n",
        "diffusion_num_batches = 1\n",
        "seed = 0\n",
        "cutn = 16\n",
        "prompts = [text]            # you can specify a different prompt for the diffusion process\n",
        "image_prompts = []\n",
        "clip_guidance_scale = 1000\n",
        "tv_scale = 0\n",
        "range_scale = 0\n",
        "stop_at = 1000\n",
        "\n",
        "\n",
        "def do_run():\n",
        "  if seed is not None:\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "  make_cutouts = MakeCutouts(clip_size, cutn)\n",
        "  side_x = side_y = model_config['image_size']\n",
        "\n",
        "  target_embeds, weights = [], []\n",
        "\n",
        "  for prompt in prompts:\n",
        "    txt, weight = parse_prompt(prompt)\n",
        "    target_embeds.append(clip_model.encode_text(clip.tokenize(prompt).to(device)).float())\n",
        "    weights.append(weight)\n",
        "\n",
        "  for prompt in image_prompts:\n",
        "    path, weight = parse_prompt(prompt)\n",
        "    img = Image.open(fetch(path)).convert('RGB')\n",
        "    img = TF.resize(img, min(side_x, side_y, *img.size), transforms.InterpolationMode.LANCZOS)\n",
        "    batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "    embed = clip_model.encode_image(normalize(batch)).float()\n",
        "    target_embeds.append(embed)\n",
        "    weights.extend([weight / cutn] * cutn)\n",
        "\n",
        "  target_embeds = torch.cat(target_embeds)\n",
        "  weights = torch.tensor(weights, device=device)\n",
        "  if weights.sum().abs() < 1e-3:\n",
        "    raise RuntimeError('The weights must not sum to 0.')\n",
        "  weights /= weights.sum().abs()\n",
        "\n",
        "  img_seq = image_tokens[image_index].unsqueeze(0)\n",
        "\n",
        "  b, n = img_seq.shape\n",
        "  one_hot_indices = F.one_hot(img_seq, num_classes = vae.num_tokens).float()\n",
        "  embeds = one_hot_indices @ vae.model.quantize.embed.weight\n",
        "\n",
        "  embeds = rearrange(embeds, 'b (h w) c -> b c h w', h = int(math.sqrt(n)))\n",
        "\n",
        "  embeds = embeds.repeat(diffusion_batch_size, 1, 1, 1)\n",
        "\n",
        "  cur_t = None\n",
        "\n",
        "  def cond_fn(x, t, image_embeds=None):\n",
        "    with torch.enable_grad():\n",
        "\n",
        "      x = x.detach().requires_grad_()\n",
        "      n = x.shape[0]\n",
        "\n",
        "      my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t\n",
        "\n",
        "      out = diffusion.p_mean_variance(model, x, my_t, clip_denoised=False, model_kwargs={'image_embeds': image_embeds})\n",
        "      fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
        "      x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
        "      clip_in = normalize(make_cutouts(x_in.add(1).div(2)))\n",
        "      clip_embeds = clip_model.encode_image(clip_in).float()\n",
        "      dists = spherical_dist_loss(clip_embeds.unsqueeze(1), target_embeds.unsqueeze(0))\n",
        "      dists = dists.view([cutn, n, -1])\n",
        "      losses = dists.mul(weights).sum(2).mean(0)\n",
        "      tv_losses = tv_loss(x_in)\n",
        "      range_losses = range_loss(out['pred_xstart'])\n",
        "      loss = losses.sum() * clip_guidance_scale + tv_losses.sum() * tv_scale + range_losses.sum() * range_scale\n",
        "      return -torch.autograd.grad(loss, x)[0]\n",
        "\n",
        "  if model_config['timestep_respacing'].startswith('ddim'):\n",
        "    sample_fn = diffusion.ddim_sample_loop_progressive\n",
        "  else:\n",
        "    sample_fn = diffusion.p_sample_loop_progressive\n",
        "\n",
        "  for i in range(diffusion_num_batches):\n",
        "    cur_t = diffusion.num_timesteps - 1\n",
        "\n",
        "    samples = sample_fn(\n",
        "      model,\n",
        "      (diffusion_batch_size, 3, side_y, side_x),\n",
        "      clip_denoised=False,\n",
        "      model_kwargs={'image_embeds': embeds},\n",
        "      cond_fn=cond_fn if clip_guidance else None,\n",
        "      progress=True,\n",
        "    )\n",
        "\n",
        "    for j, sample in enumerate(samples):\n",
        "      cur_t -= 1\n",
        "\n",
        "      if j % 100 == 0 or cur_t == -1 or j == 999 or j > stop_at:\n",
        "        for k, image in enumerate(sample['pred_xstart']):\n",
        "          pimg = TF.to_pil_image(image.add(1).div(2).clamp(0, 1))\n",
        "          display(pimg)\n",
        "      if j > stop_at:\n",
        "        break\n",
        "\n",
        "gc.collect()\n",
        "do_run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}